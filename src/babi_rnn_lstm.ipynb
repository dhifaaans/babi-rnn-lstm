{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, Dropout, Embedding\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_accuracy\n",
    "from itertools import chain\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "Trains a basic RNN and LSTM on the first five tasks of Facebook bABI.\n",
    "\n",
    "Inspiration for this code is taken from the Keras team babi_rnn file.\n",
    "\n",
    "Specifically: parse_stories and data_to_vector are taken from babi_rnn, credits\n",
    "go to the Keras team\n",
    "\n",
    "Original comes from \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\"\n",
    "http://arxiv.org/abs/1502.05698\n",
    "\n",
    "Task Number                  | FB LSTM Baseline | Keras QA\n",
    "---                          | ---              | ---\n",
    "QA1 - Single Supporting Fact | 50               | 100.0\n",
    "QA2 - Two Supporting Facts   | 20               | 50.0\n",
    "QA3 - Three Supporting Facts | 20               | 20.5\n",
    "QA4 - Two Arg. Relations     | 61               | 62.9\n",
    "QA5 - Three Arg. Relations   | 70               | 61.9\n",
    "QA6 - yes/No Questions       | 48               | 50.7\n",
    "QA7 - Counting               | 49               | 78.9\n",
    "QA8 - Lists/Sets             | 45               | 77.2\n",
    "QA9 - Simple Negation        | 64               | 64.0\n",
    "QA10 - Indefinite Knowledge  | 44               | 47.7\n",
    "QA11 - Basic Coreference     | 72               | 74.9\n",
    "QA12 - Conjunction           | 74               | 76.4\n",
    "QA13 - Compound Coreference  | 94               | 94.4\n",
    "QA14 - Time Reasoning        | 27               | 34.8\n",
    "QA15 - Basic Deduction       | 21               | 32.4\n",
    "QA16 - Basic Induction       | 23               | 50.6\n",
    "QA17 - Positional Reasoning  | 51               | 49.1\n",
    "QA18 - Size Reasoning        | 52               | 90.8\n",
    "QA19 - Path Finding          | 8                | 9.0\n",
    "QA20 - Agent's Motivations   | 91               | 90.7\n",
    "\n",
    "bAbI Project Resources:\n",
    "https://research.facebook.com/researchers/1543934539189348:\n",
    "\n",
    "\n",
    "'''\n",
    "def setup_local_files():\n",
    "    '''get files from local machine and return all training / testing text files in sorted order'''\n",
    "    path = 'tasks'\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    all_training_files = []\n",
    "    all_testing_files = []\n",
    "\n",
    "    for fn in files:\n",
    "        if 'train' in fn:\n",
    "            all_training_files.append(fn)\n",
    "        if 'test' in fn:\n",
    "            all_testing_files.append(fn)\n",
    "\n",
    "    all_training_files = np.asarray(sorted(all_training_files))\n",
    "    all_testing_files = np.asarray(sorted(all_testing_files))\n",
    "\n",
    "    print(all_training_files)\n",
    "    print(all_testing_files)\n",
    "\n",
    "    return (all_training_files,all_testing_files)\n",
    "\n",
    "# Setup local files\n",
    "all_training_files,all_testing_files = setup_local_files()\n",
    "\n",
    "def setup_dictionaries(training_files,testing_files):\n",
    "    '''take in all training / testing files and return as dictionaries\n",
    "    corresponding to tasks'''\n",
    "    training_tasks_dict = dict((k+1,v) for k,v in enumerate(training_files))\n",
    "    testing_tasks_dict = dict((k+1,v) for k,v in enumerate(testing_files))\n",
    "\n",
    "    return (training_tasks_dict,testing_tasks_dict)\n",
    "\n",
    "# Dictionary setup to grab tasks\n",
    "training_tasks_dict,testing_tasks_dict = setup_dictionaries(all_training_files,all_testing_files)\n",
    "\n",
    "def txt_to_raw(task_file):\n",
    "    '''\n",
    "    take in a specific task file and return a raw corpus\n",
    "    '''\n",
    "    with open(f'{os.getcwd()}/tasks/{task_file}', 'r') as file:\n",
    "        raw_corpus = file.readlines()\n",
    "        return raw_corpus\n",
    "\n",
    "def parse_story(story):\n",
    "    '''\n",
    "    parse the passed in raw text corpus. This is modeled from the babi_rnn source from the Keras team.\n",
    "    GitHub URL: https://github.com/keras-team/keras/blob/master/examples/babi_rnn.py\n",
    "    '''\n",
    "    related_content = []\n",
    "    data = []\n",
    "\n",
    "    for line in story:\n",
    "        line_id,line = line.split(' ',1)\n",
    "        line_id = int(line_id)\n",
    "\n",
    "        if line_id == 1:\n",
    "            related_content = []\n",
    "\n",
    "        if '\\t' in line:\n",
    "            question,answer,supporting_facts = line.split('\\t')\n",
    "            question = text_to_word_sequence(question,filters='?\\n')\n",
    "            answer = [answer]\n",
    "            substory = [ss for ss in related_content if ss]\n",
    "            data.append((substory,question,answer))\n",
    "            related_content.append('')\n",
    "        else:\n",
    "            line = text_to_word_sequence(line,filters='.\\n') + ['.']\n",
    "\n",
    "            for word in line:\n",
    "                related_content.append(word)\n",
    "    return data\n",
    "\n",
    "def get_unique_vocab(train_file,test_file):\n",
    "    '''opens up files and grabs unique vocabulary words from the text'''\n",
    "    with open(f'{os.getcwd()}/tasks/{train_file}','r') as train_file, open(f'{os.getcwd()}/tasks/{test_file}','r') as test_file:\n",
    "        raw_corpus_train = train_file.read()\n",
    "        raw_corpus_test = test_file.read()\n",
    "\n",
    "        train_tokenized = text_to_word_sequence(raw_corpus_train, filters='\\n\\t?123456789101112131415.')\n",
    "        test_tokenized = text_to_word_sequence(raw_corpus_test, filters='\\n\\t?123456789101112131415.')\n",
    "        return set(train_tokenized + test_tokenized + ['.'])\n",
    "\n",
    "def data_to_vector(data,word_dictionary,vocab_size,sentence_limit,story_maxlen,question_maxlen):\n",
    "    '''\n",
    "    Stories and questions are represented as word embeddings and the answers are one-hot encoded.\n",
    "    Takes the stories, finds unique words, and then vectorizing them into pure numeric form.\n",
    "    Each word has a numeric index which it gets replaced by!\n",
    "\n",
    "    This is modeled from the babi_rnn source from the Keras team.\n",
    "    GitHub URL: https://github.com/keras-team/keras/blob/master/examples/babi_rnn.py\n",
    "    '''\n",
    "    STORY_VECTOR,QUESTION_VECTOR,ANSWER_VECTOR = [],[],[]\n",
    "\n",
    "    for story,question,answer in data:\n",
    "        # Encode the story representations\n",
    "        STORY_VECTOR.append([word_dictionary[word] for word in story])\n",
    "        # Encode the question representations\n",
    "        QUESTION_VECTOR.append([word_dictionary[word] for word in question])\n",
    "        ANSWER_VECTOR.append(word_dictionary[answer[0].lower()])\n",
    "\n",
    "    return pad_sequences(STORY_VECTOR,maxlen=story_maxlen),pad_sequences(QUESTION_VECTOR,maxlen=question_maxlen),np.array(ANSWER_VECTOR)\n",
    "\n",
    "def zip_sq(story_training_input,question_training_input,story_testing_input,question_testing_input):\n",
    "    '''take story and question vectors and return a single\n",
    "    concatenated vector for both training and testing alongside combined max length'''\n",
    "    zipped_sq_training = list(zip(story_training_input,question_training_input))\n",
    "    zipped_sq_testing = list(zip(story_testing_input,question_testing_input))\n",
    "\n",
    "    sq_training_combined = []\n",
    "    sq_testing_combined = []\n",
    "\n",
    "    for sq in zipped_sq_training:\n",
    "        sq_training_combined.append(list(chain(sq[0],sq[1])))\n",
    "\n",
    "    for sq in zipped_sq_testing:\n",
    "        sq_testing_combined.append(list(chain(sq[0],sq[1])))\n",
    "\n",
    "    combined_maxlen = max(map(len,[sq for sq in sq_training_combined]))\n",
    "\n",
    "    return (sq_training_combined,sq_testing_combined,combined_maxlen)\n",
    "\n",
    "def build_rnn(combined_maxlen,vocab_maxlen,embedding_size,dropout_rate,learning_rate,task_num):\n",
    "    '''build and return the model to be used'''\n",
    "\n",
    "    print(f'Building, training and evaluating RNN for {task_num}\\n\\n')\n",
    "\n",
    "    rnn_model = Sequential()\n",
    "    rnn_model.add(Embedding(input_shape=combined_maxlen,input_dim=vocab_maxlen,output_dim=embedding_size))\n",
    "    rnn_model.add(SimpleRNN(50,return_sequences=True))\n",
    "    rnn_model.add(SimpleRNN(50))\n",
    "    rnn_model.add(Dropout(dropout_rate))\n",
    "    rnn_model.add(Dense(vocab_maxlen,activation='softmax'))\n",
    "\n",
    "    rnn_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    print('Build completed, returning RNN Model...')\n",
    "\n",
    "    return rnn_model\n",
    "\n",
    "def run_rnn(rnn_model,x,y,testing_x,testing_y,epochs,task_num):\n",
    "    '''build and run the rnn model and return the history'''\n",
    "\n",
    "    print(f'Training and evaluating RNN for {task_num}\\n\\n')\n",
    "\n",
    "\n",
    "    train_history = rnn_model.fit(x=np.array(x),y=np.array(y),\n",
    "                                  epochs=epochs,validation_split=0.05)\n",
    "    loss, accuracy = rnn_model.evaluate(x=np.array(testing_x),\n",
    "                                        y=np.array(testing_y),\n",
    "                                        batch_size=32)\n",
    "    print(f'\\n\\nRNN Evaluation loss: {loss}, Evaluation accuracy: {accuracy} for task {task_num}\\n\\n')\n",
    "\n",
    "    return train_history, loss, accuracy\n",
    "\n",
    "def build_lstm(combined_maxlen,vocab_maxlen,embedding_size,dropout_rate,learning_rate,task_num):\n",
    "    '''build and return the model to be used'''\n",
    "\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Embedding(input_shape=combined_maxlen,input_dim=vocab_maxlen,output_dim=embedding_size))\n",
    "    lstm_model.add(LSTM(50,return_sequences=True))\n",
    "    lstm_model.add(LSTM(50))\n",
    "    lstm_model.add(Dropout(dropout_rate))\n",
    "    lstm_model.add(Dense(vocab_maxlen, activation='softmax'))\n",
    "\n",
    "    lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    print('Build completed, returning LSTM Model...')\n",
    "\n",
    "    return lstm_model\n",
    "\n",
    "def run_lstm(lstm_model,x,y,testing_x,testing_y,epochs,task_num):\n",
    "    '''build and run the lstm model'''\n",
    "\n",
    "    print(f'Training and evaluating LSTM for {task_num}\\n\\n')\n",
    "\n",
    "\n",
    "    train_history = lstm_model.fit(np.array(x),np.array(y),\n",
    "                            epochs=epochs,validation_split=0.05)\n",
    "    loss, accuracy = lstm_model.evaluate(x=np.array(testing_x),\n",
    "                                        y=np.array(testing_y),\n",
    "                                        batch_size=32)\n",
    "    print(f'\\n\\nLSTM Evaluation loss: {loss}, Evaluation accuracy: {accuracy} for task {task_num}\\n\\n')\n",
    "\n",
    "    return train_history, loss, accuracy\n",
    "\n",
    "def predict_results(model,story_question_input,answer_testing_input):\n",
    "    '''predict and return results of prediction'''\n",
    "    def predictions_helper(expected,actuals):\n",
    "        '''given the expected answers and the actual answers compare and contrast '''\n",
    "        correct = 0\n",
    "\n",
    "        for i in range(len(expected)):\n",
    "            if expected[i] == actuals[i]:\n",
    "                correct += 1\n",
    "\n",
    "        print(f'\\n\\n----\\nOut of 1000 possible answers the model correctly predicted: {correct}')\n",
    "\n",
    "    predictions = model.predict([np.array(story_question_input)])\n",
    "\n",
    "    idxs_of_preds = []\n",
    "\n",
    "    for preds in predictions:\n",
    "        for idx,ps in enumerate(preds):\n",
    "            if ps == max(preds):\n",
    "                idxs_of_preds.append(idx)\n",
    "\n",
    "    print(f'List of all the predictions made by our Model: \\n\\n{idxs_of_preds}')\n",
    "    print(f'\\n\\n---\\n\\n List of the expected values given by our testing: \\n\\n{answer_testing_input}')\n",
    "\n",
    "    predictions_helper(answer_testing_input,idxs_of_preds)\n",
    "\n",
    "def plot_loss(training_history, model_type, task_num):\n",
    "    '''plot training vs validation loss'''\n",
    "    plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "    plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{model_type} Training loss vs Evaluation loss for task {task_num}')\n",
    "\n",
    "def plot_acc(training_history, model_type, task_num):\n",
    "    '''plot training vs validation accuracy'''\n",
    "    plt.plot(training_history.history['acc'], label='Training Accuracy')\n",
    "    plt.plot(training_history.history['val_acc'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{model_type} Training accuracy vs Evaluation accuracy for task {task_num}')\n",
    "\n",
    "def plot_all_training_losses_rnn(rnn_hist):\n",
    "    '''plot rnn training losses'''\n",
    "    rnn_loss_epoch_fig = plt.figure().add_subplot(1,1,1)\n",
    "\n",
    "    tasks = ['Single Supporting Fact', 'Two Supporting Facts', 'Three Supporting Facts',\n",
    "            'Two Arg. Relations', 'Three Arg. Relations']\n",
    "\n",
    "    for i in range(5):\n",
    "        rnn_loss_epoch_fig.plot(rnn_hist[i].history['loss'], label=f'Task {i+1} - {tasks[i]}')\n",
    "\n",
    "    rnn_loss_epoch_fig.legend()\n",
    "    rnn_loss_epoch_fig.legend(bbox_to_anchor=(1, 1))\n",
    "    rnn_loss_epoch_fig.set_xlabel('Epoch')\n",
    "    rnn_loss_epoch_fig.set_ylabel('Loss')\n",
    "    rnn_loss_epoch_fig.set_title(f'Loss rate for RNN for tasks 1 - 5 with Adam')\n",
    "\n",
    "def plot_all_training_acc_rnn(rnn_hist):\n",
    "    rnn_acc_fig = plt.figure().add_subplot(1,1,1)\n",
    "    tasks = ['Single Supporting Fact', 'Two Supporting Facts', 'Three Supporting Facts',\n",
    "            'Two Arg. Relations', 'Three Arg. Relations']\n",
    "\n",
    "    for i in range(5):\n",
    "        rnn_acc_fig.plot(rnn_hist[i].history['acc'], label=f'Task {i+1} - {tasks[i]}')\n",
    "\n",
    "    rnn_acc_fig.legend(bbox_to_anchor=(1, 1))\n",
    "    rnn_acc_fig.set_xlabel('Epoch')\n",
    "    rnn_acc_fig.set_ylabel('Accuracy')\n",
    "    rnn_acc_fig.set_title('Accuracy for RNN for tasks 1 - 5')\n",
    "\n",
    "def plot_all_training_losses_lstm(lstm_hist):\n",
    "    '''plot all lstm training losses'''\n",
    "    lstm_loss_epoch_fig = plt.figure().add_subplot(1,1,1)\n",
    "    tasks = ['Single Supporting Fact', 'Two Supporting Facts', 'Three Supporting Facts',\n",
    "            'Two Arg. Relations', 'Three Arg. Relations']\n",
    "\n",
    "    for i in range(5):\n",
    "        lstm_loss_epoch_fig.plot(lstm_hist[i].history['loss'], label=f'Task {i+1} - {tasks[i]}')\n",
    "\n",
    "    lstm_loss_epoch_fig.legend(bbox_to_anchor=(1, 1))\n",
    "    lstm_loss_epoch_fig.set_xlabel('Epoch')\n",
    "    lstm_loss_epoch_fig.set_ylabel('Loss')\n",
    "    lstm_loss_epoch_fig.set_title('Loss rate for LSTM for tasks 1 - 5 with Adam')\n",
    "\n",
    "def plot_all_training_acc_lstm(lstm_hist):\n",
    "    lstm_acc_fig = plt.figure().add_subplot(1,1,1)\n",
    "    tasks = ['Single Supporting Fact', 'Two Supporting Facts', 'Three Supporting Facts',\n",
    "            'Two Arg. Relations', 'Three Arg. Relations']\n",
    "\n",
    "    for i in range(5):\n",
    "        lstm_acc_fig.plot(lstm_hist[i].history['acc'], label=f'Task {i+1} - {tasks[i]}')\n",
    "\n",
    "    lstm_acc_fig.legend(bbox_to_anchor=(1, 1))\n",
    "    lstm_acc_fig.set_xlabel('Epoch')\n",
    "    lstm_acc_fig.set_ylabel('Accuracy')\n",
    "    lstm_acc_fig.set_title('Accuracy for LSTM for tasks 1 - 5')\n",
    "\n",
    "def run_all(embedding_size,dropout_rate,rnn_learning_rate,lstm_learning_rate,rnn_epochs,lstm_epochs):\n",
    "    '''run all tasks and return history along with evaluations'''\n",
    "    all_rnn_history = []\n",
    "    all_lstm_history = []\n",
    "    all_rnn_eval_loss = []\n",
    "    all_lstm_eval_loss = []\n",
    "    all_rnn_eval_acc = []\n",
    "    all_lstm_eval_acc = []\n",
    "\n",
    "    print('Running all tasks')\n",
    "    print(f'Passed in parameters are the following EMBEDDING SIZE: {embedding_size}, DROPOUT RATE: {dropout_rate}',\\\n",
    "          f'LEARNING RATE FOR RNN: {rnn_learning_rate}, LEARNING RATE FOR LSTM: {lstm_learning_rate},\\\n",
    "          , RNN EPOCHS: {rnn_epochs}, LSTM EPOCHS: {lstm_epochs}\\n\\n')\n",
    "\n",
    "    print('Building models...')\n",
    "\n",
    "    for task_number in range(1,6):\n",
    "        print(f'Running RNN and LSTM for Task {task_number}\\n\\n')\n",
    "        # Text to raw\n",
    "        task_training_corpus = txt_to_raw(training_tasks_dict[task_number])\n",
    "        task_testing_corpus = txt_to_raw(training_tasks_dict[task_number])\n",
    "\n",
    "        # Set up parsed stories\n",
    "        training_data = parse_story(task_training_corpus)\n",
    "        testing_data = parse_story(task_testing_corpus)\n",
    "\n",
    "        # Get unique vocabulary\n",
    "        vocab = get_unique_vocab(training_tasks_dict[task_number],testing_tasks_dict[task_number])\n",
    "\n",
    "\n",
    "        # Get max lengths\n",
    "        vocab_maxlen = len(vocab) + 1\n",
    "        story_maxlen = max(map(len,[s for s,_,_ in training_data]))\n",
    "        question_maxlen = max(map(len,[q for _,q,_ in training_data]))\n",
    "\n",
    "        # Set up word indices\n",
    "        word_index = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "        index_words = [''] + list(vocab)\n",
    "\n",
    "        # Vectorize stories, questions and answers\n",
    "        vocab_maxlen = len(vocab) + 1\n",
    "        sentence_limit = story_maxlen\n",
    "        vocab_size = vocab_maxlen\n",
    "\n",
    "        story_training_input,question_training_input,answer_training_input = data_to_vector(training_data,word_index,\n",
    "                                                                                            vocab_size,sentence_limit,\n",
    "                                                                                           story_maxlen,\n",
    "                                                                                           question_maxlen)\n",
    "        story_testing_input,question_testing_input,answer_testing_input = data_to_vector(testing_data,word_index,\n",
    "                                                                                vocab_size,sentence_limit,\n",
    "                                                                                        story_maxlen,\n",
    "                                                                                        question_maxlen)\n",
    "\n",
    "        # Zip up story, questions\n",
    "        sq_training_combined,sq_testing_combined,combined_maxlen = zip_sq(story_training_input,question_training_input,\n",
    "                                                 story_testing_input,question_testing_input)\n",
    "\n",
    "        print('Building model, training and evaluating...\\n\\n')\n",
    "        # Run and plot RNN / LSTM\n",
    "        rnn_model = build_rnn(combined_maxlen=(combined_maxlen,),vocab_maxlen=vocab_maxlen,embedding_size=embedding_size,dropout_rate=dropout_rate,\n",
    "                             learning_rate=rnn_learning_rate,task_num=task_number)\n",
    "        lstm_model = build_lstm(combined_maxlen=(combined_maxlen,),vocab_maxlen=vocab_maxlen,embedding_size=embedding_size,dropout_rate=dropout_rate,\n",
    "                               learning_rate=lstm_learning_rate,task_num=task_number)\n",
    "\n",
    "        rnn_history, rnn_eval_loss, rnn_eval_acc = run_rnn(rnn_model=rnn_model,x=sq_training_combined,\n",
    "                                                       y=answer_training_input,\n",
    "                                                       testing_x=sq_testing_combined,\n",
    "                                                       testing_y=answer_testing_input,\n",
    "                                                       epochs=rnn_epochs,task_num=task_number)\n",
    "        lstm_history, lstm_eval_loss, lstm_eval_acc = run_lstm(lstm_model=lstm_model,x=sq_training_combined,\n",
    "                                                           y=answer_training_input,testing_x=sq_testing_combined,\n",
    "                                                           testing_y=answer_testing_input,\n",
    "                                                           epochs=lstm_epochs,task_num=task_number)\n",
    "\n",
    "        # Make Predictions\n",
    "        print(f'\\n\\n RNN Model Predictions for task {task_number}\\n')\n",
    "        rnn_predictions = predict_results(rnn_model, sq_testing_combined, answer_testing_input)\n",
    "        print(f'\\n\\n LSTM Model Predictions for task {task_number}\\n')\n",
    "        lstm_predictions = predict_results(lstm_model, sq_testing_combined, answer_testing_input)\n",
    "\n",
    "        all_rnn_history.append(rnn_history)\n",
    "        all_lstm_history.append(lstm_history)\n",
    "        all_rnn_eval_loss.append(rnn_eval_loss)\n",
    "        all_rnn_eval_acc.append(rnn_eval_acc)\n",
    "        all_lstm_eval_loss.append(lstm_eval_loss)\n",
    "        all_lstm_eval_acc.append(lstm_eval_acc)\n",
    "\n",
    "        print(f'End build for task {task_number}')\n",
    "\n",
    "    return (all_rnn_history,all_lstm_history,\n",
    "           all_rnn_eval_loss,all_rnn_eval_acc,\n",
    "           all_lstm_eval_loss,all_lstm_eval_acc)\n",
    "\n",
    "\n",
    "# All history for the model runs\n",
    "all_history_evaluations = run_all(embedding_size=50,dropout_rate=0.10,rnn_learning_rate=0.0001,\n",
    "                                 lstm_learning_rate=0.001,rnn_epochs=20,lstm_epochs=30)\n",
    "\n",
    "# Separated histories for RNN / LSTM and Evaluation Loss / Accuracy\n",
    "rnn_hist,lstm_hist,rnn_eval_loss,rnn_eval_acc,lstm_eval_loss,lstm_eval_acc = all_history_evaluations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
